{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d373a-23d1-462c-89a1-1b18295d4005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T12:59:54.964789Z",
     "start_time": "2025-06-12T12:59:54.944714Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import http.client\n",
    "import requests\n",
    "import shlex\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import sys\n",
    "from datetime import datetime, timedelta, date\n",
    "from io import StringIO\n",
    "from itertools import product, chain\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "warnings.filterwarnings('ignore')\n",
    "# Note:\n",
    "    # Download BSE, NSE, BFO AND NFO after 15:30:00\n",
    "    # For CDS, run download after 17:00:00 only\n",
    "    # For MCX, run MCX_Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2e075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T12:59:56.205349Z",
     "start_time": "2025-06-12T12:59:55.380720Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def updateTickersToExchangeFile():\n",
    "    exchangeList = ['BFO', 'NFO','MCX', 'CDS'] \n",
    "    NFO_stockOptions = pd.read_csv('./extras/NFO_Sat.csv')['Symbol'].to_list()\n",
    "    BSE = ['SENSEX', 'SNSX50', 'BANKEX']\n",
    "\n",
    "    NSE = ['INDIA VIX','NIFTY 50','NIFTY BANK','NIFTY FIN SERVICE', 'NIFTY MID SELECT', 'NIFTY NEXT 50']\n",
    "    NSE = [\n",
    "            'NIFTY METAL', 'NIFTY IT', 'NIFTY ENERGY', 'NIFTY INFRA', 'NIFTY FMCG', \n",
    "            'NIFTY PHARMA', 'NIFTY AUTO', 'NIFTY MEDIA', 'NIFTY REALTY', 'NIFTY HEALTHCARE',\n",
    "            'NIFTY PSE', 'NIFTY PSU BANK', 'NIFTY SERV SECTOR', 'NIFTY PVT BANK', \n",
    "            'NIFTY 200', 'NIFTY MNC', 'LTF', 'UNITDSPR', 'RPOWER', 'SCHNEIDER',\n",
    "            'OLAELEC', 'NIFTY MIDCAP 100','NIFTY 100','NIFTY COMMODITIES',\n",
    "            'NIFTY CONSUMPTION','NIFTY MIDCAP 50','NIFTY ALPHA 50','NIFTY MIDCAP 150',\n",
    "            'NIFTY SMLCAP 50', 'NIFTY SMLCAP 100', 'NIFTY SMLCAP 250','NIFTY 500',\n",
    "            'NIFTY CPSE', 'NIFTY NEXT 50','NIFTY MIDCAP SELECT (MIDCPNIFTY)',\n",
    "            'NIFTY FINSRV25 50', 'NIFTY CONSR DURBL', 'NIFTY OIL AND GAS',\n",
    "            'NIFTY500 MULTICAP', 'NIFTY LARGEMID250', 'NIFTY200MOMENTM30', 'NIFTY TOTAL MKT',\n",
    "            'NIFTY MICROCAP250', 'NIFTY INDIA MFG', 'NIFTY IND DIGITAL', 'ARVIND', 'SANOFI',\n",
    "            'IFBIND','IFCI', 'TATAINVEST', 'BECTORFOOD', 'IONEXCHANG', 'IPCALAB',\n",
    "            'ITC', 'AWHCL', 'LIQUIDETF', 'KOPRAN', 'LICHSGFIN', 'IRFC', 'M&M',\n",
    "            'MTNL', 'CONSUMBEES', 'CENTRUM', 'MIDHANI', 'ONGC', 'ORIENTHOT','ORIENTPPR',\n",
    "            'PGHH', 'LEMONTREE', 'DIVOPPBEES', 'SAIL','ORIENTELEC', 'SIEMENS','BORORENEW',\n",
    "            'HONDAPOWER', 'THOMASCOOK', 'PHARMABEES', 'AUTOBEES','SILVERBEES','MID150BEES',\n",
    "            'NIFTYBEES', 'JUNIORBEES', 'LIQUIDBEES', 'BANKBEES', 'GOLDBEES', 'PSUBNKBEES',\n",
    "            'SHARIABEES', 'LTGILTBEES', 'ITBEES', 'INFRABEES', 'NIF100BEES', 'GIFT NIFTY',\n",
    "            'MAPMYINDIA', \"TMCV\", \"ETERNAL\", \"GMRAIRPORT\", \"IDFCFIRSTB\", \"SAMMAANCAP\", \n",
    "            \"ARVIND\", \"AXISBANK\", \"PIRAMALFIN\"\n",
    "                \n",
    "    ] + NFO_stockOptions  + NSE\n",
    "    NSE = sorted(list(set(NSE)))\n",
    "    BSE = sorted(list(set(BSE)))\n",
    "    CDS = ['USDINR','DUSDINR','EURINR','EURUSD','JPYINR','GBPINR', 'ONMIBOR']\n",
    "    BFO = ['SENSEX', 'BANKEX']\n",
    "    NFO = ['BANKNIFTY', 'FINNIFTY', 'MIDCPNIFTY', 'NIFTY']\n",
    "    MCX = ['CRUDEOIL', 'ALUMINIUM', 'COPPER', 'NICKEL', 'ZINC', \n",
    "           'LEAD','GOLD', 'SILVER', 'NATURALGAS', \n",
    "            'MCXGOLDEX', 'MCXMETLDEX', 'MCXCRUDEX', 'MCXCOPRDEX', \n",
    "           'MCXCOMPDEX','MCXBULLDEX', 'MCXENERGY', 'MCXAGRI', 'MCXMETAL', 'MCXSILVDEX']\n",
    "    \n",
    "    returnDict = dict()\n",
    "    returnDict['BSE'] = BSE\n",
    "    returnDict['NSE'] = NSE\n",
    "    currentMonth ,currentYear, currentDay = datetime.now().month, datetime.now().year, datetime.now().day\n",
    "     \n",
    "    for exchange in exchangeList:\n",
    "        zerodhaInstrumentsGetLink = 'https://api.kite.trade/instruments/' + exchange\n",
    "        df = pd.read_csv(StringIO(requests.get(zerodhaInstrumentsGetLink).text))[[\n",
    "            'instrument_token', 'tradingsymbol', 'name', 'expiry', 'segment', 'exchange'\n",
    "            ]]\n",
    "        df['name'].fillna(df['tradingsymbol'], inplace=True)\n",
    "        df['expiry'] = pd.to_datetime(df['expiry'], format='%Y-%m-%d')\n",
    "        df = df[\n",
    "                ((df['expiry'].dt.month==currentMonth)\n",
    "                & (df['expiry'].dt.year==currentYear)\n",
    "                & (df['expiry'].dt.day== currentDay))\n",
    "                | (df['expiry'].isna())\n",
    "            ].reset_index()\n",
    "\n",
    "        df = df[(df['name'].isin(MCX + CDS + BFO + NFO + NFO_stockOptions + NSE))]\n",
    "        print(exchange, end= ' ')\n",
    "        print(df['name'].unique(), end= ' ')\n",
    "        \n",
    "        \n",
    "        if(df['expiry'].isna().all()):\n",
    "            print(\"No expiry today\")\n",
    "            symbolList = [np.nan]\n",
    "        else:\n",
    "            try:\n",
    "                print(f\"Expiry : {df.reset_index(drop=True).loc[0]['expiry'].strftime('%d-%m-%Y')}\")\n",
    "            except:\n",
    "                print(f\"Expiry : {df[~(df['expiry'].isna())].reset_index(drop=True).loc[0]['expiry'].strftime('%d-%m-%Y')}\")\n",
    "            \n",
    "            if(exchange=='CDS'):\n",
    "                symbolList = CDS    \n",
    "            else:\n",
    "                symbolList = list(df['name'].unique())\n",
    "                \n",
    "        returnDict[exchange] = symbolList\n",
    "        \n",
    "    returnDf = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in returnDict.items()]))\n",
    "    returnDf = returnDf[[ 'NSE', 'NFO','BSE', 'BFO', 'CDS', 'MCX']]\n",
    "    \n",
    "    print()\n",
    "    print(returnDf)\n",
    "    returnDf.to_csv('./tickers-to-download-all-exchanges.csv', index=False)\n",
    "\n",
    "updateTickersToExchangeFile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c6bca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T12:59:56.246370Z",
     "start_time": "2025-06-12T12:59:56.246370Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# get kite connection constant\n",
    "\n",
    "def getKiteConnectionConstants():\n",
    "    # read curl request file and split all text with quotes\n",
    "    curlRequest_text_file = open('./kite-curl-request.txt', 'r')\n",
    "    curlCommandSplitList = shlex.split(curlRequest_text_file.read())\n",
    "    curlRequest_text_file.close()\n",
    "    requestUrl = ''\n",
    "    headerParamsList = []\n",
    "    \n",
    "    # get request url and headers from file\n",
    "    for i, requestParam in enumerate(curlCommandSplitList):\n",
    "        if ( requestParam == 'curl' ):\n",
    "            requestUrl = curlCommandSplitList[i+1]\n",
    "        elif ( requestParam == '-H' ):\n",
    "            keyValueTemp = curlCommandSplitList[i+1].split(': ')\n",
    "            headerParamsList.append(keyValueTemp)\n",
    "    headerParamsList = [item for sublist in headerParamsList for item in sublist]\n",
    "    headerParamsDict = {headerParamsList[i]: headerParamsList[i + 1] for i in range(0, len(headerParamsList), 2)}\n",
    "    \n",
    "    # retrive domain, connection url\n",
    "    kiteDomain, requestConnection = requestUrl.split('//')[1].split('/', 1)\n",
    "    requestConnection = '/' +requestConnection\n",
    "    requestConnectionList = requestConnection.split('&', 1)[0].split('/')\n",
    "    \n",
    "    return [kiteDomain, requestConnectionList, headerParamsDict]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c2254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf74170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T12:59:56.732846Z",
     "start_time": "2025-06-12T12:59:56.704781Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetchDataFromZerodha(row, kiteDomain, requestConnectionList, headerParamsDict):\n",
    "    \n",
    "    fromDate, toDate, expiryOfTicker, filePath = row['from_date'], row['to_date'], row['expiry'], row['file_path']\n",
    "    ticker, tickerType, tickerExchange, instrumentToken, underlyingSymbol = row['tradingsymbol'], row['segment'], row['exchange'], str(row['instrument_token']), row['name']\n",
    "    fileFullPath = filePath + ticker + '.csv'\n",
    "        \n",
    "    # read log file for selected exchange\n",
    "    exchangeLogsFilePath = './logs/' + tickerExchange + '.csv'\n",
    "    exchangeLogs = pd.read_csv(exchangeLogsFilePath)\n",
    "    \n",
    "    \n",
    "    # create chunks of from_date and to_date\n",
    "    fromToRange, chunkSize = [], 51\n",
    "    startDate, stepDate = fromDate, fromDate+timedelta(days=chunkSize-1)\n",
    "    allCandles = pd.DataFrame(columns = ['DateTime', 'Open', 'High', 'Low', 'Close', 'Volume', 'Open Interest'])\n",
    "    \n",
    "    while( stepDate <= toDate ):\n",
    "        fromToRange.append([startDate, stepDate])\n",
    "        startDate, stepDate = stepDate+timedelta(days=1), stepDate+timedelta(days=chunkSize)\n",
    "    if ( startDate <= toDate ):\n",
    "        fromToRange.append([startDate, toDate])\n",
    "   \n",
    "    # set request connection prefix\n",
    "    if ( requestConnectionList[4].isdigit() ):\n",
    "        del requestConnectionList[4]\n",
    "        requestConnectionList.insert(4, instrumentToken)\n",
    "    \n",
    "    requestConnectionPrefix = 'https://' + kiteDomain + ('/').join(requestConnectionList) + '&oi=1&from='\n",
    "    \n",
    "    print() \n",
    "    print(row.name+1, ticker, tickerType, instrumentToken)\n",
    "    print('Expiry :: ', expiryOfTicker)\n",
    "    \n",
    "    for startDate, endDate in fromToRange:\n",
    "        requestConnectionSufix = startDate.strftime('%Y-%m-%d') + '&to=' + endDate.strftime('%Y-%m-%d')\n",
    "        requestConnection = requestConnectionPrefix + requestConnectionSufix\n",
    "        kiteResponseData = requests.request('GET', requestConnection, headers=headerParamsDict, data={})\n",
    "        maxRequestRetry, requestRetryCount = 5, 0\n",
    "        \n",
    "        # send get request to zerodha for maximum given times\n",
    "        # If request fails, will not update Logs for the given Ticker\n",
    "        while ( requestRetryCount < maxRequestRetry ):\n",
    "            try:\n",
    "                candles = pd.DataFrame(\n",
    "                            json.loads(kiteResponseData.text)['data']['candles'], \n",
    "                            columns = ['DateTime', 'Open', 'High', 'Low', 'Close', 'Volume', 'Open Interest']\n",
    "                          )\n",
    "                print(requestConnectionSufix + ' : ' + str(len(candles)))\n",
    "                allCandles = pd.concat([allCandles, candles])\n",
    "                requestRetryCount = 5\n",
    "                \n",
    "            except:\n",
    "                print(f'{requestConnectionSufix} : No Candles Found')\n",
    "                requestRetryCount = requestRetryCount + 1\n",
    "                kiteResponseData.close()\n",
    "                time.sleep(1)\n",
    "                kiteResponseData = requests.request('GET', requestConnection, headers=headerParamsDict, data={})\n",
    "                if ( requestRetryCount == maxRequestRetry ):\n",
    "                    print('Maximum Retries Limit Reached')\n",
    "        \n",
    "        kiteResponseData.close()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # append data to existing file and update logs\n",
    "    if ( len(allCandles) > 0 ):\n",
    "        \n",
    "        if not ( os.path.exists(filePath) ):\n",
    "            os.makedirs(filePath)\n",
    "        \n",
    "        getDate = lambda dateAndTime: dateAndTime.split('T')[0]\n",
    "        getTime = lambda dateAndTime: dateAndTime.split('T')[-1].split('+')[0]\n",
    "        allCandles['Date'] = pd.to_datetime(allCandles['DateTime'].apply(lambda dateAndTime: getDate(dateAndTime)), format='%Y-%m-%d')\n",
    "        allCandles['Time'] = pd.to_datetime(allCandles['DateTime'].apply(lambda dateAndTime: getTime(dateAndTime)), format='%H:%M:%S').dt.strftime('%H:%M:%S')\n",
    "        allCandles['Ticker'] = ticker\n",
    "        allCandles['Expiry Date'] = expiryOfTicker if len(tickerType.split('-'))==2 else np.nan\n",
    "        allCandles = allCandles[['Ticker', 'Date', 'Time', 'Expiry Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Open Interest']]\n",
    "        \n",
    "        if ( os.path.exists(fileFullPath) ):\n",
    "            try:\n",
    "                allCandles = pd.concat([\n",
    "                    pd.read_csv(fileFullPath, parse_dates=['Date'], date_parser=(lambda x: pd.to_datetime(x, format='%Y-%m-%d'))), \n",
    "                    allCandles\n",
    "                ])\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    allCandles = pd.concat([\n",
    "                        pd.read_csv(fileFullPath, parse_dates=['Date'], date_parser=(lambda x: pd.to_datetime(x, format='%d-%m-%Y'))), \n",
    "                        allCandles\n",
    "                    ])\n",
    "                except:\n",
    "                    print(\"Mixed Values in Date Column of Original File\")\n",
    "                    old_df = pd.read_csv(fileFullPath)\n",
    "                    old_df = old_df.reset_index(drop=True)\n",
    "                    for p in range(0, len(old_df)):\n",
    "                        try:\n",
    "                            old_df.at[p, 'Date'] = pd.to_datetime(old_df.at[p, 'Date'], format='%Y-%m-%d')\n",
    "                        except:\n",
    "                            try:\n",
    "                                old_df.at[p, 'Date'] = pd.to_datetime(old_df.at[p, 'Date'], format='%d-%m-%Y')\n",
    "                            except:\n",
    "                                pass\n",
    "                    old_df = old_df[['Ticker', 'Date', 'Time', 'Expiry Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Open Interest']]\n",
    "                    allCandles = pd.concat([old_df,allCandles])\n",
    "\n",
    "        \n",
    "        # Drop Duplicates in case of exceptional cases\n",
    "        # Save the file to its path\n",
    "        allCandles = allCandles.drop_duplicates(\n",
    "            subset=['Ticker', 'Date', 'Time'],\n",
    "            keep='last'\n",
    "        )\n",
    "        allCandles = allCandles[['Ticker', 'Date', 'Time', 'Expiry Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Open Interest']]\n",
    "        allCandles.sort_values(['Date', 'Time'], inplace=True)\n",
    "        allCandles.set_index('Ticker').to_csv(fileFullPath)\n",
    "        \n",
    "        \n",
    "        # find oldest and newest date to update/add in exchange logs\n",
    "        oldestDate, newestDate = allCandles['Date'].min(), allCandles['Date'].max()\n",
    "        tickerCondition = (exchangeLogs['tradingsymbol'] == ticker) \n",
    "        \n",
    "        # if we get old logs then update else append new logs for ticker\n",
    "        if tickerCondition.any():\n",
    "            if(len(exchangeLogs.loc[tickerCondition])>1):\n",
    "                expiryCondition = (exchangeLogs.loc[tickerCondition]['expiry'].isin(allCandles['Expiry Date'].unique()))\n",
    "                tickerCondition = (tickerCondition & expiryCondition)    \n",
    "            exchangeLogs.loc[tickerCondition, 'to_date'] = newestDate.strftime('%Y-%m-%d')\n",
    "            \n",
    "        else:\n",
    "            exchangeLogs = pd.concat([\n",
    "                exchangeLogs, pd.DataFrame({\n",
    "                    'tradingsymbol': [ticker],\n",
    "                    'name': [underlyingSymbol],\n",
    "                    'segment': [tickerType],\n",
    "                    'expiry': [expiryOfTicker] if len(tickerType.split('-'))==2 else [np.nan],\n",
    "                    'from_date': [oldestDate.strftime('%Y-%m-%d')],\n",
    "                    'to_date': [newestDate.strftime('%Y-%m-%d')],\n",
    "                    'prepared_from_date': [np.nan],\n",
    "                    'prepared_to_date': [np.nan]\n",
    "                })\n",
    "            ], ignore_index=True)\n",
    "        \n",
    "        exchangeLogs.set_index('tradingsymbol').to_csv(exchangeLogsFilePath)\n",
    "    \n",
    "    # return str(instrumentToken)+'_Done'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7623b73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T12:59:57.471302Z",
     "start_time": "2025-06-12T12:59:57.465345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set exchange to download Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1928bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T13:00:11.798232Z",
     "start_time": "2025-06-12T12:59:58.114190Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## # indices present in NFO zerodha\n",
    "nfoIndicesName = ['NIFTY', 'BANKNIFTY', 'FINNIFTY', 'MIDCPNIFTY', 'NIFTYNXT50']\n",
    "\n",
    "# location where we want to save all files\n",
    "defaultSavePath = pd.read_csv('./location.csv')['Location'][0]\n",
    "\n",
    "# choose which exchange's data we want to download\n",
    "exchangeToDownload = ['NSE', 'NFO', 'BSE', 'BFO', 'CDS', 'MCX'][\n",
    "    int(input('Choose Exchange to Download Data\\n\\t1. NSE\\n\\t2. NFO\\n\\t3. BSE\\n\\t4. BFO\\n\\t5. CDS\\n\\t6. MCX\\nEnter Exchange Number :: ')) - 1\n",
    "]\n",
    "\n",
    "zerodhaInstrumentsGetLink = 'https://api.kite.trade/instruments/' + exchangeToDownload\n",
    "zerodhaInstruments = pd.read_csv(StringIO(requests.get(zerodhaInstrumentsGetLink).text))[[\n",
    "    'instrument_token', 'tradingsymbol', 'name', 'expiry', 'segment', 'exchange'\n",
    "]]\n",
    "\n",
    "zerodhaInstruments['name'].fillna(zerodhaInstruments['tradingsymbol'], inplace=True)\n",
    "\n",
    "# read tickers we want to download under individual exchange\n",
    "tickersToDownload = pd.read_csv('./tickers-to-download-all-exchanges.csv')[exchangeToDownload].dropna().to_list()\n",
    "\n",
    "tickersUnderExchange = zerodhaInstruments[\n",
    "    zerodhaInstruments['tradingsymbol' if (exchangeToDownload=='BSE' or exchangeToDownload=='NSE') else 'name'].isin(tickersToDownload)\n",
    "].set_index('instrument_token')\n",
    "\n",
    "\n",
    "\n",
    "# assign segment's name under exchange\n",
    "if ( exchangeToDownload == 'BSE' or exchangeToDownload == 'NSE' ):\n",
    "    tickersUnderExchange.loc[tickersUnderExchange['segment'] == exchangeToDownload, 'segment'] = 'STOCKS'\n",
    "elif ( exchangeToDownload == 'NFO' ):\n",
    "    tickersUnderExchange.loc[(tickersUnderExchange['segment'] == exchangeToDownload+'-FUT'), 'segment'] = 'STOCKS-FUTURE'\n",
    "    tickersUnderExchange.loc[(tickersUnderExchange['segment'] == exchangeToDownload+'-OPT'), 'segment'] = 'STOCKS-OPTION'\n",
    "    tickersUnderExchange.loc[\n",
    "        (tickersUnderExchange['segment'] == 'STOCKS-FUTURE') & (tickersUnderExchange['name'].isin(nfoIndicesName)), 'segment'\n",
    "    ] = 'INDICES-FUTURE'\n",
    "    tickersUnderExchange.loc[\n",
    "        (tickersUnderExchange['segment'] == 'STOCKS-OPTION') & (tickersUnderExchange['name'].isin(nfoIndicesName)), 'segment'\n",
    "    ] = 'INDICES-OPTION'\n",
    "else:\n",
    "    tickersUnderExchange.loc[tickersUnderExchange['segment'] == exchangeToDownload+'-FUT', 'segment'] = 'INDICES-FUTURE'\n",
    "    tickersUnderExchange.loc[tickersUnderExchange['segment'] == exchangeToDownload+'-OPT', 'segment'] = 'INDICES-OPTION'\n",
    "\n",
    "# read logs file under exchange \n",
    "# and assign from_date and to_date as per available data\n",
    "availableFiles = pd.read_csv('./logs/' + exchangeToDownload + '.csv')\n",
    "availableFiles['to_date'] = pd.to_datetime(availableFiles['to_date'], format='%Y-%m-%d')+timedelta(days=1)\n",
    "availableFiles = availableFiles.drop(columns=['from_date']).rename(columns={'to_date': 'from_date'})\n",
    "\n",
    "tickersUnderExchange['from_date'] = pd.to_datetime(\n",
    "    np.select([tickersUnderExchange['segment'].isin(['INDICES', 'STOCKS'])], [datetime(2015, 1, 1)], \n",
    "              default=date.today()-timedelta(days=100)), \n",
    "    format='%Y-%m-%d'\n",
    ")\n",
    "\n",
    "tickersUnderExchange.loc[tickersUnderExchange['segment'].isin(['INDICES', 'STOCKS']), 'name'] = tickersUnderExchange['tradingsymbol']\n",
    "\n",
    "tickersUnderExchange = tickersUnderExchange.reset_index().merge(availableFiles, on=['tradingsymbol', 'name', 'expiry', 'segment'], how='left').set_index('instrument_token')\n",
    "\n",
    "tickersUnderExchange['from_date_y'].fillna(tickersUnderExchange['from_date_x'], inplace=True)\n",
    "tickersUnderExchange = tickersUnderExchange.drop(columns=['from_date_x']).rename(columns={'from_date_y': 'from_date'})\n",
    "tickersUnderExchange['to_date'] = pd.to_datetime(date.today().strftime('%Y-%m-%d'), format='%Y-%m-%d')\n",
    "\n",
    "# create dataframe for ticker not able to download under exchange\n",
    "tickersDontUnderExchange = tickersUnderExchange[tickersUnderExchange['from_date'] > tickersUnderExchange['to_date']]\n",
    "tickersDontUnderExchange.to_csv(f'./downloads/tickers-dont-download-under-exchange-{exchangeToDownload}.csv')\n",
    "\n",
    "# Create Dataframe for ticker able to Download - sorted by Expiry to address prioritisation and avoidable data loss\n",
    "tickersUnderExchange = tickersUnderExchange[tickersUnderExchange['from_date'] <= tickersUnderExchange['to_date']]\n",
    "tickersUnderExchange['expiry'] = pd.to_datetime(tickersUnderExchange['expiry'], format='%Y-%m-%d', errors=\"coerce\")\n",
    "tickersUnderExchange = tickersUnderExchange.sort_values(\n",
    "                            by=['expiry','segment','name', 'tradingsymbol'], \n",
    "                            na_position='first')\n",
    "\n",
    "# Get current month expiry Data in Stock-Option on Expiry Day\n",
    "# Relevant when downloading Stock Options Data along with others\n",
    "tickersUnderExchange = tickersUnderExchange[    \n",
    "                            (\n",
    "                                (tickersUnderExchange['expiry'].dt.month == datetime.now().month) \n",
    "                                & (tickersUnderExchange['expiry'].dt.year == datetime.now().year)\n",
    "                                & (tickersUnderExchange['expiry'].dt.day == datetime.now().day)\n",
    "                            ) \n",
    "                            | (tickersUnderExchange['expiry'].isna()) \n",
    "                       ]\n",
    "\n",
    "tickersUnderExchange['expiry'] = tickersUnderExchange['expiry'].dt.strftime('%Y-%m-%d')\n",
    "tickersUnderExchange.to_csv(f'./downloads/tickers-to-download-under-exchange-{exchangeToDownload}.csv')\n",
    "\n",
    "# create file path for individual tickers\n",
    "filePathSufix = np.select(\n",
    "    [tickersUnderExchange['segment'].isin(['INDICES-FUTURE', 'STOCKS-FUTURE']), tickersUnderExchange['segment'].isin(['INDICES-OPTION', 'STOCKS-OPTION'])], \n",
    "    [tickersUnderExchange['name'] + '/', tickersUnderExchange['name'] + '/' + tickersUnderExchange['expiry'] + '/'], \n",
    "    default=''\n",
    ")\n",
    "tickersUnderExchange['file_path'] = defaultSavePath + tickersUnderExchange['exchange'] + '/' + tickersUnderExchange['segment'] + '/' + filePathSufix\n",
    "\n",
    "tickersUnderExchange\n",
    "# print(tickersUnderExchange.shape)\n",
    "# create zerodha kite connection\n",
    "kiteDomain, requestConnectionList, headerParamsDict = getKiteConnectionConstants()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b83cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tickersUnderExchange = pd.read_csv(\"./downloads/tickers-to-download-under-exchange-NSE.csv\")\n",
    "# # print(tickersUnderExchange)\n",
    "# filePathSufix = np.select(\n",
    "#     [tickersUnderExchange['segment'].isin(['INDICES-FUTURE', 'STOCKS-FUTURE']), tickersUnderExchange['segment'].isin(['INDICES-OPTION', 'STOCKS-OPTION'])], \n",
    "#     [tickersUnderExchange['name'] + '/', tickersUnderExchange['name'] + '/' + tickersUnderExchange['expiry'] + '/'], \n",
    "#     default=''\n",
    "# )\n",
    "# tickersUnderExchange['file_path'] = defaultSavePath + tickersUnderExchange['exchange'] + '/' + tickersUnderExchange['segment'] + '/' + filePathSufix\n",
    "# tickersUnderExchange['from_date'] = pd.to_datetime(tickersUnderExchange['from_date'], format='%Y-%m-%d')\n",
    "# tickersUnderExchange['to_date'] = pd.to_datetime(tickersUnderExchange['to_date'], format='%Y-%m-%d')\n",
    "# print(tickersUnderExchange)\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "retries = 0\n",
    "i = 0\n",
    "df = tickersUnderExchange.reset_index().copy(deep=True)\n",
    "while i < len(df):\n",
    "    row = df.iloc[i]\n",
    "    try:\n",
    "        fetchDataFromZerodha(row, kiteDomain, requestConnectionList, headerParamsDict)\n",
    "        i += 1\n",
    "        retries = 0  # reset after success\n",
    "    except Exception as e:\n",
    "        retries += 1\n",
    "        print(f\"Error at index {i} (retry {retries}/{MAX_RETRIES}): {e}\")\n",
    "\n",
    "        if retries >= MAX_RETRIES:\n",
    "            print(f\"Skipping index {i} after max retries\")\n",
    "            i += 1\n",
    "            retries = 0\n",
    "print(\"All Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e353005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
